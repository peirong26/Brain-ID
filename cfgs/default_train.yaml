

## paths
out_root: yogurt # yogurt or lemon
root_dir_lemon: /autofs/vast/lemon/temp_stuff/peirong # mlsc run
root_dir_yogurt_out: /autofs/space/yogurt_002/users/pl629 # desktop run
out_dir: results/BrainID


supervised_seg_cnn_weights: /autofs/space/yogurt_002/users/pl629/ckp/supervised_segmentation_model.pth
feat_ext_ckp_path:
ckp_path:

device:
device_generator: 
device_segmenter: 


task: feat-anat # choices: feature training: { feat-anat, feat-seg, feat-anat-seg }, downstream task training: { seg, sr, reg, bf }
task_f_maps: [64]


## losses and weights
losses:
  image_grad: True
  image_grad_mask: False
  bias_field_log_type: l2 # l1 or l2 

weights:
  seg_ce: 1.
  seg_dice: 1.
  dist: 1.
  image: 1.
  image_grad: 1.
  seg_supervised: 1.
  bias_field_log: 1.
  reg: 1.
  reg_grad: 0.01
  contrastive: 1.


## training params
start_epoch: 0
train_itr_limit: # if not None, it sets the max itr per epoch 
train_subset: #0.2

resume: False
reset_epoch: False
resume_optim: True
resume_lr_scheduler: True
freeze_feat: False


batch_size: 1

n_epochs: 400
lr_scheduler: multistep # cosine, multistep
lr_drops: [250,300]
lr_drop_multi: 0.1
lr: 0.0001 # Learning rate at the end of linear warmup (highest LR used during training)
min_lr: 0.000001 # Target LR at the end of optimization. We use a cosine LR schedule with linear warmup
warmup_epochs: 1 # Number of epochs for the linear learning-rate warm up

feat_opt:  
  lr: 0.0001 # Learning rate at the end of linear warmup (highest LR used during training)
  min_lr: 0.000001 # Target LR at the end of optimization. We use a cosine LR schedule with linear warmup 


optimizer: adamw # adam, adamw
weight_decay: 0 # 0.04 # Final value of the weight decay. A cosine schedule for WD and using a larger decay by the end of training improves performance for ViTs.
weight_decay_end: 0 # 0.4 # Final value of the weight decay. A cosine schedule for WD and using a larger decay by the end of training improves performance for ViTs.
momentum: 1 # 1 as disabling momentum

# gradient clipping max norm
clip_max_norm: 0.
freeze_last_layer: 0 #Number of epochs during which we keep the output layer fixed. Typically doing so during the first epoch helps training. Try increasing this value if the loss does not decrease



## testing params
eval_only: False
debug: False 
test_itr_limit: 
test_subset:

 

seed: 42 
# distributed training parameters
# number of distributed processes
world_size: 1
# url used to set up distributed training
dist_url: env://


# Backbone
backbone: unet3d # options: unet2d, unet3d, res_unet3d, res_unet3d_se

# UNet seting 
in_channels: 1 
f_maps: 64 
f_maps_supervised_seg_cnn: 32
num_groups: 8
num_levels: 5
layer_order: 'gcl'
final_sigmoid: False

## UNDER TESTING ##
relative_weight_lesions: 1.0 # for now...


## visualizer params
visualizer:
  make_results: False
  save_image: False
  spacing: [8, 8, 8] 

  feat_vis: True
  feat_vis_num: 10 # fixed number of feature channels to plot

## logging intervals
val_epoch: 100000 # must be multiplicable by save_model_epoch

log_itr: 20
vis_itr: 1000